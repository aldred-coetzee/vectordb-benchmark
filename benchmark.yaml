# Shared Benchmark Configuration
# This file defines datasets, index types, and search parameters for all databases

# =============================================================================
# Datasets
# =============================================================================

datasets:
  sift:
    path: data/sift
    vectors: 1000000
    dimensions: 128
    metric: L2
    format: fvecs
    name: "SIFT-1M"
    source: "ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz"
    purpose: "Baseline — the standard ANN benchmark. Low dimension (128D), L2 distance. Allows direct comparison with published results from other benchmarking tools."
    description: "1M SIFT image descriptors extracted from the Flickr dataset. 128-dimensional local feature vectors widely used as the canonical vector search benchmark since 2008."

  gist:
    path: data/gist
    vectors: 1000000
    dimensions: 960
    metric: L2
    format: fvecs
    name: "GIST-1M"
    source: "ftp://ftp.irisa.fr/local/texmex/corpus/gist.tar.gz"
    purpose: "Dimension stress — 960D vectors test memory bandwidth, cache pressure, and curse-of-dimensionality effects. Exposes database payload limits."
    description: "1M GIST global image descriptors. 960-dimensional vectors — 7.5x larger than SIFT — that stress high-dimensional indexing."

  glove-100:
    path: data/glove-100/glove-100-angular.hdf5
    vectors: 1183514
    dimensions: 100
    metric: cosine
    format: hdf5
    name: "GloVe-100"
    source: "http://ann-benchmarks.com/glove-100-angular.hdf5"
    purpose: "Cosine metric — most production vector search workloads use cosine or inner-product distance, not L2. Tests native angular distance support and whether databases incur overhead converting between metrics."
    description: "1.18M GloVe word embedding vectors (100D) trained on Wikipedia + Gigaword. Angular (cosine) distance. The standard dataset for testing non-L2 metrics in ANN benchmarks."

  dbpedia-openai:
    path: data/dbpedia-openai/dbpedia-openai-1000k-angular.hdf5
    vectors: 990000
    dimensions: 1536
    metric: cosine
    format: hdf5
    name: "DBpedia-OpenAI-1M"
    source: "https://storage.googleapis.com/ann-datasets/ann-benchmarks/dbpedia-openai-1000k-angular.hdf5"
    purpose: "Production-representative — 1536D OpenAI text-embedding-ada-002 vectors match the dimensionality of the most widely deployed embedding model. Tests real-world RAG/search workload performance at high dimensionality with cosine distance."
    description: "990K DBpedia entity embeddings generated with OpenAI text-embedding-ada-002 (1536D). Angular (cosine) distance. Represents production RAG and semantic search workloads."

  # Dev datasets — small subsets for fast iteration (NOT for benchmarking)
  # Generate with: python scripts/generate_dev_datasets.py
  sift-dev:
    path: data/sift-dev
    vectors: 10000
    dimensions: 128
    metric: L2
    format: fvecs
    name: "SIFT-dev"
    description: "Dev subset of SIFT-1M (10K vectors, 100 queries). For code testing only."

  gist-dev:
    path: data/gist-dev
    vectors: 10000
    dimensions: 960
    metric: L2
    format: fvecs
    name: "GIST-dev"
    description: "Dev subset of GIST-1M (10K vectors, 100 queries). For code testing only."

# =============================================================================
# Index Types
# =============================================================================

indexes:
  flat:
    description: "Brute-force exact search. Compares the query against every vector in the dataset. Guarantees 100% recall but is slow on large datasets. Used as a baseline to validate recall and measure raw scan throughput."
    params: {}

  hnsw:
    description: "Hierarchical Navigable Small World graph — the dominant approximate nearest neighbor (ANN) index. Builds a multi-layer proximity graph at ingest time, then traverses it at search time. Trades recall for speed: higher efSearch improves recall but reduces QPS."
    M: 16
    efConstruction: 64
    efSearch:
      - 32
      - 64
      - 128
      - 256

  ivf:
    description: "Inverted File index. Partitions vectors into clusters and searches only nearby clusters. Not yet implemented."
    params: {}

  ivfpq:
    description: "IVF with Product Quantization. Combines cluster-based partitioning with vector compression for memory efficiency. Not yet implemented."
    params: {}

# =============================================================================
# Search Configuration
# =============================================================================

search:
  k: 10
  num_queries: 10000
  warmup: 100
  batch_size: 50000

# =============================================================================
# Metric Descriptions (used in report generation)
# =============================================================================

metrics:
  qps:
    name: "Queries Per Second (QPS)"
    description: "Number of single-vector queries completed per second. Queries are executed sequentially (one at a time, wait for response). Measures single-client throughput, not concurrent load."

  recall_at_10:
    name: "Recall@10"
    description: "Fraction of true 10 nearest neighbors found by the search. Computed against brute-force ground truth provided with each dataset. 1.0 = perfect accuracy."

  recall_at_100:
    name: "Recall@100"
    description: "Fraction of true 100 nearest neighbors found. A stricter accuracy measure than Recall@10."

  latency_p50:
    name: "P50 Latency (ms)"
    description: "Median query latency. 50% of queries complete faster than this value."

  latency_p99:
    name: "P99 Latency (ms)"
    description: "99th percentile query latency. Measures tail latency — the worst-case experience for 1 in 100 queries."

  ingest_throughput:
    name: "Ingest Throughput (vectors/sec)"
    description: "Rate of vector insertion including index building. For HNSW, each insert updates the graph, so throughput decreases as the dataset grows."

  batch_qps:
    name: "Batch QPS"
    description: "Queries per second when all queries are sent in a single API call. Measures throughput under batch workloads. Available for 5/9 databases (FAISS, Qdrant, Milvus, ChromaDB, KDB.AI). Per-query latency percentiles are not available — the database processes queries together."

  ef_search:
    name: "efSearch"
    description: "HNSW search beam width — the number of candidate nodes evaluated at each step of graph traversal. Higher values explore more of the graph, improving recall at the cost of QPS. Swept across [32, 64, 128, 256] to produce a recall-vs-speed tradeoff curve."

  m_parameter:
    name: "M (HNSW)"
    description: "Maximum number of edges per node in the HNSW graph. Fixed at M=16 across all databases for fair comparison. Higher M improves recall and increases memory usage."

  ef_construction:
    name: "efConstruction (HNSW)"
    description: "Beam width used during HNSW graph construction. Fixed at 64 across all databases. Higher values build a better quality graph at the cost of slower ingest."

# =============================================================================
# Infrastructure
# =============================================================================

infrastructure:
  repo: "https://github.com/aldred-coetzee/vectordb-benchmark"
  aws:
    region: us-west-2
    instance_type: m5.4xlarge
    vcpus: 16
    memory_gb: 64
    os: "Amazon Linux 2023"
